{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV as LGCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Read in, split 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in dataset\n",
    "amazon = pd.read_csv('amazon_need.csv')\n",
    "\n",
    "#mapping __label__1 to 1 and __label__2 to 0 as instructions\n",
    "amazon['label'] = amazon.label.map(lambda x: 1 if x == '__label__1' else 0)\n",
    "\n",
    "x = amazon['review']\n",
    "y = amazon['label']\n",
    "\n",
    "#splitting data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size = .7, random_state = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Train random forests on: bag of words, TF-IDF, Word2Vec (avg), Word2Vec (sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(ngram_range = (1,1), stop_words = \"english\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using fit_transform to fit training reviews & create dict matrix \n",
    "train_counts = count_vector.fit_transform(x_train)\n",
    "\n",
    "#creating DF from dict matrix created above\n",
    "train_bag = pd.DataFrame(train_counts.toarray(), columns=count_vector.get_feature_names())\n",
    "\n",
    "#finding word occurence in df\n",
    "word_counts = train_bag.sum()\n",
    "\n",
    "#creating variable of only words that have an occurence of at least 20\n",
    "freq_train_bag = train_bag[word_counts[word_counts >= 20].index]\n",
    "\n",
    "#standardizing \n",
    "freq_train_bag = (freq_train_bag - freq_train_bag.mean()) / freq_train_bag.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using transform on validation reviews, not fit_transform as it was fit on the training data\n",
    "val_counts = count_vector.transform(x_val)\n",
    "\n",
    "#similar as above, but for val set \n",
    "val_bag = pd.DataFrame(val_counts.toarray(), columns=count_vector.get_feature_names())\n",
    "\n",
    "#reducing\n",
    "freq_val_train = val_bag[freq_train_bag.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters - smaller than usual ranges b/c computation time was excessive\n",
    "parameters = {\"n_estimators\":[100,200,300,400], \"min_samples_leaf\": [5,10,15,20,25,30], \"min_samples_split\": [10,20,30,40,50], 'max_depth': [2,4,6,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating rforest to plug into randomizedsearchcv. using verbose=10 b/c I like validation that the model is running.\n",
    "rforest = RandomForestClassifier(random_state=13, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating randomizedsearchcv with given params, using rforest.\n",
    "rforest_bow = RandomizedSearchCV(rforest, parameters, n_jobs=4, n_iter=13, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 300\n",
      "building tree 2 of 300\n",
      "building tree 3 of 300\n",
      "building tree 4 of 300\n",
      "building tree 5 of 300\n",
      "building tree 6 of 300\n",
      "building tree 7 of 300\n",
      "building tree 8 of 300\n",
      "building tree 9 of 300\n",
      "building tree 10 of 300\n",
      "building tree 11 of 300\n",
      "building tree 12 of 300\n",
      "building tree 13 of 300\n",
      "building tree 14 of 300\n",
      "building tree 15 of 300\n",
      "building tree 16 of 300\n",
      "building tree 17 of 300\n",
      "building tree 18 of 300\n",
      "building tree 19 of 300\n",
      "building tree 20 of 300\n",
      "building tree 21 of 300\n",
      "building tree 22 of 300\n",
      "building tree 23 of 300\n",
      "building tree 24 of 300\n",
      "building tree 25 of 300\n",
      "building tree 26 of 300\n",
      "building tree 27 of 300\n",
      "building tree 28 of 300\n",
      "building tree 29 of 300\n",
      "building tree 30 of 300\n",
      "building tree 31 of 300\n",
      "building tree 32 of 300\n",
      "building tree 33 of 300\n",
      "building tree 34 of 300\n",
      "building tree 35 of 300\n",
      "building tree 36 of 300\n",
      "building tree 37 of 300\n",
      "building tree 38 of 300\n",
      "building tree 39 of 300\n",
      "building tree 40 of 300\n",
      "building tree 41 of 300\n",
      "building tree 42 of 300\n",
      "building tree 43 of 300\n",
      "building tree 44 of 300\n",
      "building tree 45 of 300\n",
      "building tree 46 of 300\n",
      "building tree 47 of 300\n",
      "building tree 48 of 300\n",
      "building tree 49 of 300\n",
      "building tree 50 of 300\n",
      "building tree 51 of 300\n",
      "building tree 52 of 300\n",
      "building tree 53 of 300\n",
      "building tree 54 of 300\n",
      "building tree 55 of 300\n",
      "building tree 56 of 300\n",
      "building tree 57 of 300\n",
      "building tree 58 of 300\n",
      "building tree 59 of 300\n",
      "building tree 60 of 300\n",
      "building tree 61 of 300\n",
      "building tree 62 of 300\n",
      "building tree 63 of 300\n",
      "building tree 64 of 300\n",
      "building tree 65 of 300\n",
      "building tree 66 of 300\n",
      "building tree 67 of 300\n",
      "building tree 68 of 300\n",
      "building tree 69 of 300\n",
      "building tree 70 of 300\n",
      "building tree 71 of 300\n",
      "building tree 72 of 300\n",
      "building tree 73 of 300\n",
      "building tree 74 of 300\n",
      "building tree 75 of 300\n",
      "building tree 76 of 300\n",
      "building tree 77 of 300\n",
      "building tree 78 of 300\n",
      "building tree 79 of 300\n",
      "building tree 80 of 300\n",
      "building tree 81 of 300\n",
      "building tree 82 of 300\n",
      "building tree 83 of 300\n",
      "building tree 84 of 300\n",
      "building tree 85 of 300\n",
      "building tree 86 of 300\n",
      "building tree 87 of 300\n",
      "building tree 88 of 300\n",
      "building tree 89 of 300\n",
      "building tree 90 of 300\n",
      "building tree 91 of 300\n",
      "building tree 92 of 300\n",
      "building tree 93 of 300\n",
      "building tree 94 of 300\n",
      "building tree 95 of 300\n",
      "building tree 96 of 300\n",
      "building tree 97 of 300\n",
      "building tree 98 of 300\n",
      "building tree 99 of 300\n",
      "building tree 100 of 300\n",
      "building tree 101 of 300\n",
      "building tree 102 of 300\n",
      "building tree 103 of 300\n",
      "building tree 104 of 300\n",
      "building tree 105 of 300\n",
      "building tree 106 of 300\n",
      "building tree 107 of 300\n",
      "building tree 108 of 300\n",
      "building tree 109 of 300\n",
      "building tree 110 of 300\n",
      "building tree 111 of 300\n",
      "building tree 112 of 300\n",
      "building tree 113 of 300\n",
      "building tree 114 of 300\n",
      "building tree 115 of 300\n",
      "building tree 116 of 300\n",
      "building tree 117 of 300\n",
      "building tree 118 of 300\n",
      "building tree 119 of 300\n",
      "building tree 120 of 300\n",
      "building tree 121 of 300\n",
      "building tree 122 of 300\n",
      "building tree 123 of 300\n",
      "building tree 124 of 300\n",
      "building tree 125 of 300\n",
      "building tree 126 of 300\n",
      "building tree 127 of 300\n",
      "building tree 128 of 300\n",
      "building tree 129 of 300\n",
      "building tree 130 of 300\n",
      "building tree 131 of 300\n",
      "building tree 132 of 300\n",
      "building tree 133 of 300\n",
      "building tree 134 of 300\n",
      "building tree 135 of 300\n",
      "building tree 136 of 300\n",
      "building tree 137 of 300\n",
      "building tree 138 of 300\n",
      "building tree 139 of 300\n",
      "building tree 140 of 300\n",
      "building tree 141 of 300\n",
      "building tree 142 of 300\n",
      "building tree 143 of 300\n",
      "building tree 144 of 300\n",
      "building tree 145 of 300\n",
      "building tree 146 of 300\n",
      "building tree 147 of 300\n",
      "building tree 148 of 300\n",
      "building tree 149 of 300\n",
      "building tree 150 of 300\n",
      "building tree 151 of 300\n",
      "building tree 152 of 300\n",
      "building tree 153 of 300\n",
      "building tree 154 of 300\n",
      "building tree 155 of 300\n",
      "building tree 156 of 300\n",
      "building tree 157 of 300\n",
      "building tree 158 of 300\n",
      "building tree 159 of 300\n",
      "building tree 160 of 300\n",
      "building tree 161 of 300\n",
      "building tree 162 of 300\n",
      "building tree 163 of 300\n",
      "building tree 164 of 300\n",
      "building tree 165 of 300\n",
      "building tree 166 of 300\n",
      "building tree 167 of 300\n",
      "building tree 168 of 300\n",
      "building tree 169 of 300\n",
      "building tree 170 of 300\n",
      "building tree 171 of 300\n",
      "building tree 172 of 300\n",
      "building tree 173 of 300\n",
      "building tree 174 of 300\n",
      "building tree 175 of 300\n",
      "building tree 176 of 300\n",
      "building tree 177 of 300\n",
      "building tree 178 of 300\n",
      "building tree 179 of 300\n",
      "building tree 180 of 300\n",
      "building tree 181 of 300\n",
      "building tree 182 of 300\n",
      "building tree 183 of 300\n",
      "building tree 184 of 300\n",
      "building tree 185 of 300\n",
      "building tree 186 of 300\n",
      "building tree 187 of 300\n",
      "building tree 188 of 300\n",
      "building tree 189 of 300\n",
      "building tree 190 of 300\n",
      "building tree 191 of 300\n",
      "building tree 192 of 300\n",
      "building tree 193 of 300\n",
      "building tree 194 of 300\n",
      "building tree 195 of 300\n",
      "building tree 196 of 300\n",
      "building tree 197 of 300\n",
      "building tree 198 of 300\n",
      "building tree 199 of 300\n",
      "building tree 200 of 300\n",
      "building tree 201 of 300\n",
      "building tree 202 of 300\n",
      "building tree 203 of 300\n",
      "building tree 204 of 300\n",
      "building tree 205 of 300\n",
      "building tree 206 of 300\n",
      "building tree 207 of 300\n",
      "building tree 208 of 300\n",
      "building tree 209 of 300\n",
      "building tree 210 of 300\n",
      "building tree 211 of 300\n",
      "building tree 212 of 300\n",
      "building tree 213 of 300\n",
      "building tree 214 of 300\n",
      "building tree 215 of 300\n",
      "building tree 216 of 300\n",
      "building tree 217 of 300\n",
      "building tree 218 of 300\n",
      "building tree 219 of 300\n",
      "building tree 220 of 300\n",
      "building tree 221 of 300\n",
      "building tree 222 of 300\n",
      "building tree 223 of 300\n",
      "building tree 224 of 300\n",
      "building tree 225 of 300\n",
      "building tree 226 of 300\n",
      "building tree 227 of 300\n",
      "building tree 228 of 300\n",
      "building tree 229 of 300\n",
      "building tree 230 of 300\n",
      "building tree 231 of 300\n",
      "building tree 232 of 300\n",
      "building tree 233 of 300\n",
      "building tree 234 of 300\n",
      "building tree 235 of 300\n",
      "building tree 236 of 300\n",
      "building tree 237 of 300\n",
      "building tree 238 of 300\n",
      "building tree 239 of 300\n",
      "building tree 240 of 300\n",
      "building tree 241 of 300\n",
      "building tree 242 of 300\n",
      "building tree 243 of 300\n",
      "building tree 244 of 300\n",
      "building tree 245 of 300\n",
      "building tree 246 of 300\n",
      "building tree 247 of 300\n",
      "building tree 248 of 300\n",
      "building tree 249 of 300\n",
      "building tree 250 of 300\n",
      "building tree 251 of 300\n",
      "building tree 252 of 300\n",
      "building tree 253 of 300\n",
      "building tree 254 of 300\n",
      "building tree 255 of 300\n",
      "building tree 256 of 300\n",
      "building tree 257 of 300\n",
      "building tree 258 of 300\n",
      "building tree 259 of 300\n",
      "building tree 260 of 300\n",
      "building tree 261 of 300\n",
      "building tree 262 of 300\n",
      "building tree 263 of 300\n",
      "building tree 264 of 300\n",
      "building tree 265 of 300\n",
      "building tree 266 of 300\n",
      "building tree 267 of 300\n",
      "building tree 268 of 300\n",
      "building tree 269 of 300\n",
      "building tree 270 of 300\n",
      "building tree 271 of 300\n",
      "building tree 272 of 300\n",
      "building tree 273 of 300\n",
      "building tree 274 of 300\n",
      "building tree 275 of 300\n",
      "building tree 276 of 300\n",
      "building tree 277 of 300\n",
      "building tree 278 of 300\n",
      "building tree 279 of 300\n",
      "building tree 280 of 300\n",
      "building tree 281 of 300\n",
      "building tree 282 of 300\n",
      "building tree 283 of 300\n",
      "building tree 284 of 300\n",
      "building tree 285 of 300\n",
      "building tree 286 of 300\n",
      "building tree 287 of 300\n",
      "building tree 288 of 300\n",
      "building tree 289 of 300\n",
      "building tree 290 of 300\n",
      "building tree 291 of 300\n",
      "building tree 292 of 300\n",
      "building tree 293 of 300\n",
      "building tree 294 of 300\n",
      "building tree 295 of 300\n",
      "building tree 296 of 300\n",
      "building tree 297 of 300\n",
      "building tree 298 of 300\n",
      "building tree 299 of 300\n",
      "building tree 300 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(random_state=13,\n",
       "                                                    verbose=10),\n",
       "                   n_iter=13, n_jobs=4,\n",
       "                   param_distributions={'max_depth': [2, 4, 6, 8],\n",
       "                                        'min_samples_leaf': [5, 10, 15, 20, 25,\n",
       "                                                             30],\n",
       "                                        'min_samples_split': [10, 20, 30, 40,\n",
       "                                                              50],\n",
       "                                        'n_estimators': [100, 200, 300, 400]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WILL RUN FOR ~3 MIN!\n",
    "#training RFmodel on the \"frequent\" bag of words and their corresponding label.\n",
    "rforest_bow.fit(freq_train_bag, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators parameter for the model above is 300\n",
      "The best min_samples_split parameter for the model above is 30\n",
      "The best min_samples_leaf parameter for the model above is 5\n",
      "The best max_depth parameter for the model above is 6\n"
     ]
    }
   ],
   "source": [
    "params = rforest_bow.best_params_\n",
    "for x, y in params.items():\n",
    "    print('The best', x, 'parameter for the model above is', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#finding how the model's predicted probabilities of the words' labels\n",
    "y_train_prob = rforest_bow.predict_proba(freq_train_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding AUC of the training set. \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "tbow_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#Finding predicted probabilities of the validation dataset using model trained on training data.\n",
    "y_val_prob = rforest_bow.predict_proba(freq_val_train)\n",
    "\n",
    "#Finding AUC of the validation set. \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "vbow_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BOW training data set had an AUC score  of 0.902240804129517 while the BOW validation data set had an AUC score of 0.7256553455865971 for a difference of: 0.17658545854291996\n"
     ]
    }
   ],
   "source": [
    "print('The BOW training data set had an AUC score  of', tbow_auc, \n",
    "      'while the BOW validation data set had an AUC score of', vbow_auc, 'for a difference of:', tbow_auc - vbow_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great</td>\n",
       "      <td>0.052642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waste</td>\n",
       "      <td>0.044548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money</td>\n",
       "      <td>0.037662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.033966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>0.030147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boring</td>\n",
       "      <td>0.030025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>best</td>\n",
       "      <td>0.030007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.022225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bad</td>\n",
       "      <td>0.021670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>0.020243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.020030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.019680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.019618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>poor</td>\n",
       "      <td>0.017672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>don</td>\n",
       "      <td>0.017266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>awful</td>\n",
       "      <td>0.016238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.015455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>0.013269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>return</td>\n",
       "      <td>0.013156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>0.013015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>poorly</td>\n",
       "      <td>0.012819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>highly</td>\n",
       "      <td>0.012625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>junk</td>\n",
       "      <td>0.011823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loved</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>didn</td>\n",
       "      <td>0.010894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>returned</td>\n",
       "      <td>0.010872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>0.010241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.009661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>crap</td>\n",
       "      <td>0.009317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               var  importance\n",
       "0            great    0.052642\n",
       "1            waste    0.044548\n",
       "2            money    0.037662\n",
       "3            worst    0.033966\n",
       "4             love    0.030147\n",
       "5           boring    0.030025\n",
       "6             best    0.030007\n",
       "7        excellent    0.022225\n",
       "8              bad    0.021670\n",
       "9     disappointed    0.020243\n",
       "10        terrible    0.020030\n",
       "11        horrible    0.019680\n",
       "12       wonderful    0.019618\n",
       "13            poor    0.017672\n",
       "14             don    0.017266\n",
       "15           awful    0.016238\n",
       "16         awesome    0.015455\n",
       "17   disappointing    0.013269\n",
       "18          return    0.013156\n",
       "19  disappointment    0.013015\n",
       "20          poorly    0.012819\n",
       "21          highly    0.012625\n",
       "22            junk    0.011823\n",
       "23           loved    0.011001\n",
       "24            didn    0.010894\n",
       "25        returned    0.010872\n",
       "26         perfect    0.010289\n",
       "27       beautiful    0.010241\n",
       "28            easy    0.009661\n",
       "29            crap    0.009317"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 30 features\n",
    "feats = pd.DataFrame()\n",
    "feats[\"var\"] = freq_train_bag.columns\n",
    "feats[\"importance\"] = rforest_bow.best_estimator_.feature_importances_\n",
    "feats = feats.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "feats[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating TF-IDF vectorizer using unigram\n",
    "tf = TfidfVectorizer(ngram_range = (1,1), stop_words = \"english\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting TF-IDF vectorizer on training data reviews\n",
    "train_tf = tf.fit_transform(x_train)\n",
    "\n",
    "#taking dict matrix of words/value into an array\n",
    "train_tf = pd.DataFrame(train_tf.toarray(), columns = tf.get_feature_names())\n",
    "\n",
    "#again selecting only words that were used at least 20 times\n",
    "train_tf = train_tf[word_counts[word_counts >= 20].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above, but for validation data, and using transform, not fit_transform. \n",
    "val_tf = tf.transform(x_val)\n",
    "val_tf = pd.DataFrame(val_tf.toarray(), columns = tf.get_feature_names())\n",
    "val_tf = val_tf[word_counts[word_counts >= 20].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as done previously, using randomizedsearch on rforest, saved under new variable for my ease to keep best params\n",
    "#per text mining model\n",
    "rforest_tf = RandomizedSearchCV(rforest, parameters, n_jobs=4, n_iter= 13, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 400\n",
      "building tree 2 of 400\n",
      "building tree 3 of 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 4 of 400\n",
      "building tree 5 of 400\n",
      "building tree 6 of 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 7 of 400\n",
      "building tree 8 of 400\n",
      "building tree 9 of 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 10 of 400\n",
      "building tree 11 of 400\n",
      "building tree 12 of 400\n",
      "building tree 13 of 400\n",
      "building tree 14 of 400\n",
      "building tree 15 of 400\n",
      "building tree 16 of 400\n",
      "building tree 17 of 400\n",
      "building tree 18 of 400\n",
      "building tree 19 of 400\n",
      "building tree 20 of 400\n",
      "building tree 21 of 400\n",
      "building tree 22 of 400\n",
      "building tree 23 of 400\n",
      "building tree 24 of 400\n",
      "building tree 25 of 400\n",
      "building tree 26 of 400\n",
      "building tree 27 of 400\n",
      "building tree 28 of 400\n",
      "building tree 29 of 400\n",
      "building tree 30 of 400\n",
      "building tree 31 of 400\n",
      "building tree 32 of 400\n",
      "building tree 33 of 400\n",
      "building tree 34 of 400\n",
      "building tree 35 of 400\n",
      "building tree 36 of 400\n",
      "building tree 37 of 400\n",
      "building tree 38 of 400\n",
      "building tree 39 of 400\n",
      "building tree 40 of 400\n",
      "building tree 41 of 400\n",
      "building tree 42 of 400\n",
      "building tree 43 of 400\n",
      "building tree 44 of 400\n",
      "building tree 45 of 400\n",
      "building tree 46 of 400\n",
      "building tree 47 of 400\n",
      "building tree 48 of 400\n",
      "building tree 49 of 400\n",
      "building tree 50 of 400\n",
      "building tree 51 of 400\n",
      "building tree 52 of 400\n",
      "building tree 53 of 400\n",
      "building tree 54 of 400\n",
      "building tree 55 of 400\n",
      "building tree 56 of 400\n",
      "building tree 57 of 400\n",
      "building tree 58 of 400\n",
      "building tree 59 of 400\n",
      "building tree 60 of 400\n",
      "building tree 61 of 400\n",
      "building tree 62 of 400\n",
      "building tree 63 of 400\n",
      "building tree 64 of 400\n",
      "building tree 65 of 400\n",
      "building tree 66 of 400\n",
      "building tree 67 of 400\n",
      "building tree 68 of 400\n",
      "building tree 69 of 400\n",
      "building tree 70 of 400\n",
      "building tree 71 of 400\n",
      "building tree 72 of 400\n",
      "building tree 73 of 400\n",
      "building tree 74 of 400\n",
      "building tree 75 of 400\n",
      "building tree 76 of 400\n",
      "building tree 77 of 400\n",
      "building tree 78 of 400\n",
      "building tree 79 of 400\n",
      "building tree 80 of 400\n",
      "building tree 81 of 400\n",
      "building tree 82 of 400\n",
      "building tree 83 of 400\n",
      "building tree 84 of 400\n",
      "building tree 85 of 400\n",
      "building tree 86 of 400\n",
      "building tree 87 of 400\n",
      "building tree 88 of 400\n",
      "building tree 89 of 400\n",
      "building tree 90 of 400\n",
      "building tree 91 of 400\n",
      "building tree 92 of 400\n",
      "building tree 93 of 400\n",
      "building tree 94 of 400\n",
      "building tree 95 of 400\n",
      "building tree 96 of 400\n",
      "building tree 97 of 400\n",
      "building tree 98 of 400\n",
      "building tree 99 of 400\n",
      "building tree 100 of 400\n",
      "building tree 101 of 400\n",
      "building tree 102 of 400\n",
      "building tree 103 of 400\n",
      "building tree 104 of 400\n",
      "building tree 105 of 400\n",
      "building tree 106 of 400\n",
      "building tree 107 of 400\n",
      "building tree 108 of 400\n",
      "building tree 109 of 400\n",
      "building tree 110 of 400\n",
      "building tree 111 of 400\n",
      "building tree 112 of 400\n",
      "building tree 113 of 400\n",
      "building tree 114 of 400\n",
      "building tree 115 of 400\n",
      "building tree 116 of 400\n",
      "building tree 117 of 400\n",
      "building tree 118 of 400\n",
      "building tree 119 of 400\n",
      "building tree 120 of 400\n",
      "building tree 121 of 400\n",
      "building tree 122 of 400\n",
      "building tree 123 of 400\n",
      "building tree 124 of 400\n",
      "building tree 125 of 400\n",
      "building tree 126 of 400\n",
      "building tree 127 of 400\n",
      "building tree 128 of 400\n",
      "building tree 129 of 400\n",
      "building tree 130 of 400\n",
      "building tree 131 of 400\n",
      "building tree 132 of 400\n",
      "building tree 133 of 400\n",
      "building tree 134 of 400\n",
      "building tree 135 of 400\n",
      "building tree 136 of 400\n",
      "building tree 137 of 400\n",
      "building tree 138 of 400\n",
      "building tree 139 of 400\n",
      "building tree 140 of 400\n",
      "building tree 141 of 400\n",
      "building tree 142 of 400\n",
      "building tree 143 of 400\n",
      "building tree 144 of 400\n",
      "building tree 145 of 400\n",
      "building tree 146 of 400\n",
      "building tree 147 of 400\n",
      "building tree 148 of 400\n",
      "building tree 149 of 400\n",
      "building tree 150 of 400\n",
      "building tree 151 of 400\n",
      "building tree 152 of 400\n",
      "building tree 153 of 400\n",
      "building tree 154 of 400\n",
      "building tree 155 of 400\n",
      "building tree 156 of 400\n",
      "building tree 157 of 400\n",
      "building tree 158 of 400\n",
      "building tree 159 of 400\n",
      "building tree 160 of 400\n",
      "building tree 161 of 400\n",
      "building tree 162 of 400\n",
      "building tree 163 of 400\n",
      "building tree 164 of 400\n",
      "building tree 165 of 400\n",
      "building tree 166 of 400\n",
      "building tree 167 of 400\n",
      "building tree 168 of 400\n",
      "building tree 169 of 400\n",
      "building tree 170 of 400\n",
      "building tree 171 of 400\n",
      "building tree 172 of 400\n",
      "building tree 173 of 400\n",
      "building tree 174 of 400\n",
      "building tree 175 of 400\n",
      "building tree 176 of 400\n",
      "building tree 177 of 400\n",
      "building tree 178 of 400\n",
      "building tree 179 of 400\n",
      "building tree 180 of 400\n",
      "building tree 181 of 400\n",
      "building tree 182 of 400\n",
      "building tree 183 of 400\n",
      "building tree 184 of 400\n",
      "building tree 185 of 400\n",
      "building tree 186 of 400\n",
      "building tree 187 of 400\n",
      "building tree 188 of 400\n",
      "building tree 189 of 400\n",
      "building tree 190 of 400\n",
      "building tree 191 of 400\n",
      "building tree 192 of 400\n",
      "building tree 193 of 400\n",
      "building tree 194 of 400\n",
      "building tree 195 of 400\n",
      "building tree 196 of 400\n",
      "building tree 197 of 400\n",
      "building tree 198 of 400\n",
      "building tree 199 of 400\n",
      "building tree 200 of 400\n",
      "building tree 201 of 400\n",
      "building tree 202 of 400\n",
      "building tree 203 of 400\n",
      "building tree 204 of 400\n",
      "building tree 205 of 400\n",
      "building tree 206 of 400\n",
      "building tree 207 of 400\n",
      "building tree 208 of 400\n",
      "building tree 209 of 400\n",
      "building tree 210 of 400\n",
      "building tree 211 of 400\n",
      "building tree 212 of 400\n",
      "building tree 213 of 400\n",
      "building tree 214 of 400\n",
      "building tree 215 of 400\n",
      "building tree 216 of 400\n",
      "building tree 217 of 400\n",
      "building tree 218 of 400\n",
      "building tree 219 of 400\n",
      "building tree 220 of 400\n",
      "building tree 221 of 400\n",
      "building tree 222 of 400\n",
      "building tree 223 of 400\n",
      "building tree 224 of 400\n",
      "building tree 225 of 400\n",
      "building tree 226 of 400\n",
      "building tree 227 of 400\n",
      "building tree 228 of 400\n",
      "building tree 229 of 400\n",
      "building tree 230 of 400\n",
      "building tree 231 of 400\n",
      "building tree 232 of 400\n",
      "building tree 233 of 400\n",
      "building tree 234 of 400\n",
      "building tree 235 of 400\n",
      "building tree 236 of 400\n",
      "building tree 237 of 400\n",
      "building tree 238 of 400\n",
      "building tree 239 of 400\n",
      "building tree 240 of 400\n",
      "building tree 241 of 400\n",
      "building tree 242 of 400\n",
      "building tree 243 of 400\n",
      "building tree 244 of 400\n",
      "building tree 245 of 400\n",
      "building tree 246 of 400\n",
      "building tree 247 of 400\n",
      "building tree 248 of 400\n",
      "building tree 249 of 400\n",
      "building tree 250 of 400\n",
      "building tree 251 of 400\n",
      "building tree 252 of 400\n",
      "building tree 253 of 400\n",
      "building tree 254 of 400\n",
      "building tree 255 of 400\n",
      "building tree 256 of 400\n",
      "building tree 257 of 400\n",
      "building tree 258 of 400\n",
      "building tree 259 of 400\n",
      "building tree 260 of 400\n",
      "building tree 261 of 400\n",
      "building tree 262 of 400\n",
      "building tree 263 of 400\n",
      "building tree 264 of 400\n",
      "building tree 265 of 400\n",
      "building tree 266 of 400\n",
      "building tree 267 of 400\n",
      "building tree 268 of 400\n",
      "building tree 269 of 400\n",
      "building tree 270 of 400\n",
      "building tree 271 of 400\n",
      "building tree 272 of 400\n",
      "building tree 273 of 400\n",
      "building tree 274 of 400\n",
      "building tree 275 of 400\n",
      "building tree 276 of 400\n",
      "building tree 277 of 400\n",
      "building tree 278 of 400\n",
      "building tree 279 of 400\n",
      "building tree 280 of 400\n",
      "building tree 281 of 400\n",
      "building tree 282 of 400\n",
      "building tree 283 of 400\n",
      "building tree 284 of 400\n",
      "building tree 285 of 400\n",
      "building tree 286 of 400\n",
      "building tree 287 of 400\n",
      "building tree 288 of 400\n",
      "building tree 289 of 400\n",
      "building tree 290 of 400\n",
      "building tree 291 of 400\n",
      "building tree 292 of 400\n",
      "building tree 293 of 400\n",
      "building tree 294 of 400\n",
      "building tree 295 of 400\n",
      "building tree 296 of 400\n",
      "building tree 297 of 400\n",
      "building tree 298 of 400\n",
      "building tree 299 of 400\n",
      "building tree 300 of 400\n",
      "building tree 301 of 400\n",
      "building tree 302 of 400\n",
      "building tree 303 of 400\n",
      "building tree 304 of 400\n",
      "building tree 305 of 400\n",
      "building tree 306 of 400\n",
      "building tree 307 of 400\n",
      "building tree 308 of 400\n",
      "building tree 309 of 400\n",
      "building tree 310 of 400\n",
      "building tree 311 of 400\n",
      "building tree 312 of 400\n",
      "building tree 313 of 400\n",
      "building tree 314 of 400\n",
      "building tree 315 of 400\n",
      "building tree 316 of 400\n",
      "building tree 317 of 400\n",
      "building tree 318 of 400\n",
      "building tree 319 of 400\n",
      "building tree 320 of 400\n",
      "building tree 321 of 400\n",
      "building tree 322 of 400\n",
      "building tree 323 of 400\n",
      "building tree 324 of 400\n",
      "building tree 325 of 400\n",
      "building tree 326 of 400\n",
      "building tree 327 of 400\n",
      "building tree 328 of 400\n",
      "building tree 329 of 400\n",
      "building tree 330 of 400\n",
      "building tree 331 of 400\n",
      "building tree 332 of 400\n",
      "building tree 333 of 400\n",
      "building tree 334 of 400\n",
      "building tree 335 of 400\n",
      "building tree 336 of 400\n",
      "building tree 337 of 400\n",
      "building tree 338 of 400\n",
      "building tree 339 of 400\n",
      "building tree 340 of 400\n",
      "building tree 341 of 400\n",
      "building tree 342 of 400\n",
      "building tree 343 of 400\n",
      "building tree 344 of 400\n",
      "building tree 345 of 400\n",
      "building tree 346 of 400\n",
      "building tree 347 of 400\n",
      "building tree 348 of 400\n",
      "building tree 349 of 400\n",
      "building tree 350 of 400\n",
      "building tree 351 of 400\n",
      "building tree 352 of 400\n",
      "building tree 353 of 400\n",
      "building tree 354 of 400\n",
      "building tree 355 of 400\n",
      "building tree 356 of 400\n",
      "building tree 357 of 400\n",
      "building tree 358 of 400\n",
      "building tree 359 of 400\n",
      "building tree 360 of 400\n",
      "building tree 361 of 400\n",
      "building tree 362 of 400\n",
      "building tree 363 of 400\n",
      "building tree 364 of 400\n",
      "building tree 365 of 400\n",
      "building tree 366 of 400\n",
      "building tree 367 of 400\n",
      "building tree 368 of 400\n",
      "building tree 369 of 400\n",
      "building tree 370 of 400\n",
      "building tree 371 of 400\n",
      "building tree 372 of 400\n",
      "building tree 373 of 400\n",
      "building tree 374 of 400\n",
      "building tree 375 of 400\n",
      "building tree 376 of 400\n",
      "building tree 377 of 400\n",
      "building tree 378 of 400\n",
      "building tree 379 of 400\n",
      "building tree 380 of 400\n",
      "building tree 381 of 400\n",
      "building tree 382 of 400\n",
      "building tree 383 of 400\n",
      "building tree 384 of 400\n",
      "building tree 385 of 400\n",
      "building tree 386 of 400\n",
      "building tree 387 of 400\n",
      "building tree 388 of 400\n",
      "building tree 389 of 400\n",
      "building tree 390 of 400\n",
      "building tree 391 of 400\n",
      "building tree 392 of 400\n",
      "building tree 393 of 400\n",
      "building tree 394 of 400\n",
      "building tree 395 of 400\n",
      "building tree 396 of 400\n",
      "building tree 397 of 400\n",
      "building tree 398 of 400\n",
      "building tree 399 of 400\n",
      "building tree 400 of 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   33.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(random_state=13,\n",
       "                                                    verbose=10),\n",
       "                   n_iter=13, n_jobs=4,\n",
       "                   param_distributions={'max_depth': [2, 4, 6, 8],\n",
       "                                        'min_samples_leaf': [5, 10, 15, 20, 25,\n",
       "                                                             30],\n",
       "                                        'min_samples_split': [10, 20, 30, 40,\n",
       "                                                              50],\n",
       "                                        'n_estimators': [100, 200, 300, 400]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WILL RUN FOR ~5 MIN!\n",
    "#fitting rforest model on training frequent words TF with their labels\n",
    "rforest_tf.fit(train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators parameter for the model above is 400\n",
      "The best min_samples_split parameter for the model above is 10\n",
      "The best min_samples_leaf parameter for the model above is 5\n",
      "The best max_depth parameter for the model above is 8\n"
     ]
    }
   ],
   "source": [
    "params = rforest_tf.best_params_\n",
    "for x, y in params.items():\n",
    "    print('The best', x, 'parameter for the model above is', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "#getting predicted probs for training dataset\n",
    "y_train_prob = rforest_tf.predict_proba(train_tf)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "ttf_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#same as above, for validation set\n",
    "y_val_prob = rforest_tf.predict_proba(val_tf)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "vtf_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF training data set had an AUC score  of 0.9143265353116066 while the TF-IDF validation data set had an AUC score of 0.8990273107611724 for a difference of: 0.015299224550434198\n"
     ]
    }
   ],
   "source": [
    "print('The TF-IDF training data set had an AUC score  of', ttf_auc, \n",
    "      'while the TF-IDF validation data set had an AUC score of', vtf_auc, 'for a difference of:', ttf_auc - vtf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great</td>\n",
       "      <td>0.060761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waste</td>\n",
       "      <td>0.042106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money</td>\n",
       "      <td>0.036572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best</td>\n",
       "      <td>0.036047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>0.028370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bad</td>\n",
       "      <td>0.025616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>don</td>\n",
       "      <td>0.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boring</td>\n",
       "      <td>0.023455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.020934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>0.019779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.019370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>poor</td>\n",
       "      <td>0.019248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>0.017311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>awful</td>\n",
       "      <td>0.016101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.014865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.014091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.013024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>highly</td>\n",
       "      <td>0.012627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>0.011755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>favorite</td>\n",
       "      <td>0.011606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>poorly</td>\n",
       "      <td>0.011518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>return</td>\n",
       "      <td>0.010821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>junk</td>\n",
       "      <td>0.010225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>instead</td>\n",
       "      <td>0.009869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.009664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.009347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>didn</td>\n",
       "      <td>0.008867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>good</td>\n",
       "      <td>0.008690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               var  importance\n",
       "0            great    0.060761\n",
       "1            waste    0.042106\n",
       "2            money    0.036572\n",
       "3             best    0.036047\n",
       "4            worst    0.029400\n",
       "5             love    0.028370\n",
       "6              bad    0.025616\n",
       "7              don    0.023704\n",
       "8           boring    0.023455\n",
       "9         terrible    0.020934\n",
       "10    disappointed    0.019779\n",
       "11       excellent    0.019370\n",
       "12            poor    0.019248\n",
       "13   disappointing    0.017311\n",
       "14       wonderful    0.016541\n",
       "15           awful    0.016101\n",
       "16        horrible    0.014865\n",
       "17         awesome    0.014091\n",
       "18         perfect    0.013024\n",
       "19          highly    0.012627\n",
       "20  disappointment    0.011755\n",
       "21        favorite    0.011606\n",
       "22          poorly    0.011518\n",
       "23          return    0.010821\n",
       "24            junk    0.010225\n",
       "25         instead    0.009869\n",
       "26            easy    0.009664\n",
       "27         amazing    0.009347\n",
       "28            didn    0.008867\n",
       "29            good    0.008690"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 30 features\n",
    "feats[\"var\"] = freq_train_bag.columns\n",
    "feats[\"importance\"] = rforest_tf.best_estimator_.feature_importances_\n",
    "feats = feats.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "feats[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations needed for using word2vec\n",
    "#conda install -c anaconda gensim \n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "#was necessary for me b/c I'm using an older python version \n",
    "import smart_open\n",
    "smart_open.open = smart_open.smart_open\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using wordnet's lemmatizer: \"dogs\" -> \"dog\", \"cacti\" -> \"cactus\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Creating function to clean text, using from Professor's notebook with some minimal changes \n",
    "def text_cleaner(x):\n",
    "    x = x.lower()\n",
    "    #using word_tokenize to separate words & punct into their own tokens\n",
    "    tokens = nltk.tokenize.word_tokenize(x)\n",
    "    #for loop - takes all tokens that were made from above & only have alpha characters to lemmatize them.\n",
    "    lem_alpha_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    #for loop - runs through the lemmatized alpha-only characters & only keeps tokens that are not a stop word \n",
    "    lem_alpha_tokens = [token for token in lem_alpha_tokens if token not in nltk.corpus.stopwords.words(\"english\")]\n",
    "    return lem_alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using map function to take each row of reviews & clean them as per the function above\n",
    "x_train_clean = x_train.map(text_cleaner)\n",
    "\n",
    "#same as above, but for validation set reviews\n",
    "x_val_clean = x_val.map(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating word embedding vectors for the train and val sets\n",
    "\n",
    "word2vec_train = Word2Vec(x_train_clean, min_count = 5, size = 300, window = 4)\n",
    "word2vec_val = Word2Vec(x_val_clean, min_count = 5, size = 300, window = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:01<00:00, 8963.92it/s]\n",
      "100%|| 7500/7500 [00:00<00:00, 9606.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_avg_embedding(word2vec):\n",
    "    #for loop - finding avg embedding of each word; using Prof code\n",
    "    train_avg_embeddings = []\n",
    "\n",
    "    #setting for loop to go through the # of rows of cleaned words\n",
    "    for index in tqdm(range(x_train_clean.shape[0])):\n",
    "        doc = x_train_clean.iloc[index]\n",
    "\n",
    "        #assigns words(aliased by index) to their embeddings - skips words not counted for my word2vec model (those with min_count <5)\n",
    "        embeddings = [word2vec.wv[word] for word in doc if word in word2vec.wv]\n",
    "\n",
    "        #if no words gave embeddings from model above, create an empty list with same size used in word2vec model\n",
    "        if embeddings == []:\n",
    "            train_avg_embeddings.append([np.nan] * 300)\n",
    "\n",
    "        #if there are embeddings, find the mean of each word(axis=0) and add it to list\n",
    "        else:\n",
    "            avg = np.mean(embeddings, axis = 0)\n",
    "            train_avg_embeddings.append(avg)\n",
    "    return train_avg_embeddings\n",
    "\n",
    "            \n",
    "#same as above, but for validation dataset            \n",
    "def val_avg_embedding(word2vec):\n",
    "    val_avg_embeddings = []\n",
    "    for index in tqdm(range(x_val_clean.shape[0])):\n",
    "        doc = x_val_clean.iloc[index]\n",
    "        embeddings = [word2vec.wv[word] for word in doc if word in word2vec.wv]\n",
    "        if embeddings == []:\n",
    "            val_avg_embeddings.append([np.nan] * 300)\n",
    "        else:\n",
    "            avg = np.mean(embeddings, axis = 0)\n",
    "            val_avg_embeddings.append(avg)\n",
    "    return val_avg_embeddings\n",
    "\n",
    "train_avg_embeddings = train_avg_embedding(word2vec_train)\n",
    "val_avg_embeddings = val_avg_embedding(word2vec_val)\n",
    "\n",
    "word2vec_features_train = pd.DataFrame(train_avg_embeddings)\n",
    "word2vec_features_val = pd.DataFrame(val_avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RForest using randomized search as previously done\n",
    "rforest_w2v = RandomizedSearchCV(rforest, parameters, n_jobs=4, n_iter=13, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 200\n",
      "building tree 2 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 200\n",
      "building tree 4 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 5 of 200\n",
      "building tree 6 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 7 of 200\n",
      "building tree 8 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 9 of 200\n",
      "building tree 10 of 200\n",
      "building tree 11 of 200\n",
      "building tree 12 of 200\n",
      "building tree 13 of 200\n",
      "building tree 14 of 200\n",
      "building tree 15 of 200\n",
      "building tree 16 of 200\n",
      "building tree 17 of 200\n",
      "building tree 18 of 200\n",
      "building tree 19 of 200\n",
      "building tree 20 of 200\n",
      "building tree 21 of 200\n",
      "building tree 22 of 200\n",
      "building tree 23 of 200\n",
      "building tree 24 of 200\n",
      "building tree 25 of 200\n",
      "building tree 26 of 200\n",
      "building tree 27 of 200\n",
      "building tree 28 of 200\n",
      "building tree 29 of 200\n",
      "building tree 30 of 200\n",
      "building tree 31 of 200\n",
      "building tree 32 of 200\n",
      "building tree 33 of 200\n",
      "building tree 34 of 200\n",
      "building tree 35 of 200\n",
      "building tree 36 of 200\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 42 of 200\n",
      "building tree 43 of 200\n",
      "building tree 44 of 200\n",
      "building tree 45 of 200\n",
      "building tree 46 of 200\n",
      "building tree 47 of 200\n",
      "building tree 48 of 200\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "building tree 51 of 200\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 54 of 200\n",
      "building tree 55 of 200\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "building tree 59 of 200\n",
      "building tree 60 of 200\n",
      "building tree 61 of 200\n",
      "building tree 62 of 200\n",
      "building tree 63 of 200\n",
      "building tree 64 of 200\n",
      "building tree 65 of 200\n",
      "building tree 66 of 200\n",
      "building tree 67 of 200\n",
      "building tree 68 of 200\n",
      "building tree 69 of 200\n",
      "building tree 70 of 200\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 73 of 200\n",
      "building tree 74 of 200\n",
      "building tree 75 of 200\n",
      "building tree 76 of 200\n",
      "building tree 77 of 200\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "building tree 82 of 200\n",
      "building tree 83 of 200\n",
      "building tree 84 of 200\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 90 of 200\n",
      "building tree 91 of 200\n",
      "building tree 92 of 200\n",
      "building tree 93 of 200\n",
      "building tree 94 of 200\n",
      "building tree 95 of 200\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "building tree 102 of 200\n",
      "building tree 103 of 200\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n",
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "building tree 116 of 200\n",
      "building tree 117 of 200\n",
      "building tree 118 of 200\n",
      "building tree 119 of 200\n",
      "building tree 120 of 200\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n",
      "building tree 126 of 200\n",
      "building tree 127 of 200\n",
      "building tree 128 of 200\n",
      "building tree 129 of 200\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "building tree 133 of 200\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "building tree 144 of 200\n",
      "building tree 145 of 200\n",
      "building tree 146 of 200\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 149 of 200\n",
      "building tree 150 of 200\n",
      "building tree 151 of 200\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n",
      "building tree 154 of 200\n",
      "building tree 155 of 200\n",
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "building tree 158 of 200\n",
      "building tree 159 of 200\n",
      "building tree 160 of 200\n",
      "building tree 161 of 200\n",
      "building tree 162 of 200\n",
      "building tree 163 of 200\n",
      "building tree 164 of 200\n",
      "building tree 165 of 200\n",
      "building tree 166 of 200\n",
      "building tree 167 of 200\n",
      "building tree 168 of 200\n",
      "building tree 169 of 200\n",
      "building tree 170 of 200\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "building tree 173 of 200\n",
      "building tree 174 of 200\n",
      "building tree 175 of 200\n",
      "building tree 176 of 200\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "building tree 181 of 200\n",
      "building tree 182 of 200\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "building tree 185 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "building tree 188 of 200\n",
      "building tree 189 of 200\n",
      "building tree 190 of 200\n",
      "building tree 191 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200\n",
      "building tree 196 of 200\n",
      "building tree 197 of 200\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "building tree 200 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   24.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(random_state=13,\n",
       "                                                    verbose=10),\n",
       "                   n_iter=13, n_jobs=4,\n",
       "                   param_distributions={'max_depth': [2, 4, 6, 8],\n",
       "                                        'min_samples_leaf': [5, 10, 15, 20, 25,\n",
       "                                                             30],\n",
       "                                        'min_samples_split': [10, 20, 30, 40,\n",
       "                                                              50],\n",
       "                                        'n_estimators': [100, 200, 300, 400]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training/fitting model using training word embeddings and their corresponding labels\n",
    "rforest_w2v.fit(word2vec_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators parameter for the model above is 200\n",
      "The best min_samples_split parameter for the model above is 50\n",
      "The best min_samples_leaf parameter for the model above is 20\n",
      "The best max_depth parameter for the model above is 8\n"
     ]
    }
   ],
   "source": [
    "params = rforest_w2v.best_params_\n",
    "for x, y in params.items():\n",
    "    print('The best', x, 'parameter for the model above is', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "#finding how model above would predict labels for training data\n",
    "y_train_prob = rforest_w2v.predict_proba(word2vec_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "tw2v_avg_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# get predicted probabilities on validation set\n",
    "y_val_prob = rforest_w2v.predict_proba(word2vec_features_val)\n",
    "\n",
    "# Get AUC on the validation set\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "vw2v_avg_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The W2V training data set had an AUC score  of 0.8829909503399664 while the W2V validation data set had an AUC score of 0.6735071616800817 for a difference of: 0.2094837886598847\n"
     ]
    }
   ],
   "source": [
    "print('The W2V training data set had an AUC score  of', tw2v_avg_auc, \n",
    "      'while the W2V validation data set had an AUC score of', vw2v_avg_auc, 'for a difference of:', tw2v_avg_auc - vw2v_avg_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262</td>\n",
       "      <td>0.043867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>0.038970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222</td>\n",
       "      <td>0.032065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196</td>\n",
       "      <td>0.032027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>0.029327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>0.026743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>231</td>\n",
       "      <td>0.025766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>217</td>\n",
       "      <td>0.024426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>0.022051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>0.019337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68</td>\n",
       "      <td>0.018694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>208</td>\n",
       "      <td>0.018337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>0.018261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>96</td>\n",
       "      <td>0.017595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>0.017299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44</td>\n",
       "      <td>0.014896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>99</td>\n",
       "      <td>0.014399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>65</td>\n",
       "      <td>0.012569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>291</td>\n",
       "      <td>0.012263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>152</td>\n",
       "      <td>0.012212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>0.012073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>92</td>\n",
       "      <td>0.011233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>0.009603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.009532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>283</td>\n",
       "      <td>0.009368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.009365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>205</td>\n",
       "      <td>0.009258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>249</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>144</td>\n",
       "      <td>0.008774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26</td>\n",
       "      <td>0.007072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var  importance\n",
       "0   262    0.043867\n",
       "1   155    0.038970\n",
       "2   222    0.032065\n",
       "3   196    0.032027\n",
       "4    76    0.029327\n",
       "5   111    0.026743\n",
       "6   231    0.025766\n",
       "7   217    0.024426\n",
       "8    30    0.022051\n",
       "9   128    0.019337\n",
       "10   68    0.018694\n",
       "11  208    0.018337\n",
       "12  256    0.018261\n",
       "13   96    0.017595\n",
       "14   10    0.017299\n",
       "15   44    0.014896\n",
       "16   99    0.014399\n",
       "17   65    0.012569\n",
       "18  291    0.012263\n",
       "19  152    0.012212\n",
       "20   17    0.012073\n",
       "21   92    0.011233\n",
       "22  100    0.009603\n",
       "23   23    0.009532\n",
       "24  283    0.009368\n",
       "25   25    0.009365\n",
       "26  205    0.009258\n",
       "27  249    0.009071\n",
       "28  144    0.008774\n",
       "29   26    0.007072"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 30 features\n",
    "blah = pd.DataFrame()\n",
    "blah['var'] = word2vec_features_train.columns\n",
    "blah['importance'] = rforest_w2v.best_estimator_.feature_importances_\n",
    "blah = blah.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "blah[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:01<00:00, 9074.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#Next few cells repeat same process as modeling above, however, instead of finding the avg of the embeddings/word,\n",
    "#the sum is found for each. \n",
    "\n",
    "train_sum_embeddings = []\n",
    "\n",
    "for index in tqdm(range(x_train_clean.shape[0])):\n",
    "    doc = x_train_clean.iloc[index]\n",
    "    embeddings = [word2vec_train.wv[word] for word in doc if word in word2vec_train.wv]\n",
    "    if embeddings == []:\n",
    "        train_sum_embeddings.append([np.nan] * 300)\n",
    "    else:\n",
    "        summ = np.sum(embeddings, axis = 0)\n",
    "        train_sum_embeddings.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7500/7500 [00:00<00:00, 9604.95it/s]\n"
     ]
    }
   ],
   "source": [
    "val_sum_embeddings = []\n",
    "\n",
    "for index in tqdm(range(x_val_clean.shape[0])):\n",
    "    doc = x_val_clean.iloc[index]\n",
    "    embeddings = [word2vec_val.wv[word] for word in doc if word in word2vec_val.wv]\n",
    "    if embeddings == []:\n",
    "        val_sum_embeddings.append([np.nan] * 300)\n",
    "    else:\n",
    "        # otherwise, just get the mean componentwise across all the word embeddings and append to our list\n",
    "        summ = np.sum(embeddings, axis = 0)\n",
    "        val_sum_embeddings.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_features_train_sum = pd.DataFrame(train_sum_embeddings)\n",
    "word2vec_features_val_sum = pd.DataFrame(val_sum_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest_w2v_sum = RandomizedSearchCV(rforest, parameters, n_jobs=4, n_iter=13, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 300\n",
      "building tree 2 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 300\n",
      "building tree 4 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 5 of 300\n",
      "building tree 6 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 7 of 300\n",
      "building tree 8 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 9 of 300\n",
      "building tree 10 of 300\n",
      "building tree 11 of 300\n",
      "building tree 12 of 300\n",
      "building tree 13 of 300\n",
      "building tree 14 of 300\n",
      "building tree 15 of 300\n",
      "building tree 16 of 300\n",
      "building tree 17 of 300\n",
      "building tree 18 of 300\n",
      "building tree 19 of 300\n",
      "building tree 20 of 300\n",
      "building tree 21 of 300\n",
      "building tree 22 of 300\n",
      "building tree 23 of 300\n",
      "building tree 24 of 300\n",
      "building tree 25 of 300\n",
      "building tree 26 of 300\n",
      "building tree 27 of 300\n",
      "building tree 28 of 300\n",
      "building tree 29 of 300\n",
      "building tree 30 of 300\n",
      "building tree 31 of 300\n",
      "building tree 32 of 300\n",
      "building tree 33 of 300\n",
      "building tree 34 of 300\n",
      "building tree 35 of 300\n",
      "building tree 36 of 300\n",
      "building tree 37 of 300\n",
      "building tree 38 of 300\n",
      "building tree 39 of 300\n",
      "building tree 40 of 300\n",
      "building tree 41 of 300\n",
      "building tree 42 of 300\n",
      "building tree 43 of 300\n",
      "building tree 44 of 300\n",
      "building tree 45 of 300\n",
      "building tree 46 of 300\n",
      "building tree 47 of 300\n",
      "building tree 48 of 300\n",
      "building tree 49 of 300\n",
      "building tree 50 of 300\n",
      "building tree 51 of 300\n",
      "building tree 52 of 300\n",
      "building tree 53 of 300\n",
      "building tree 54 of 300\n",
      "building tree 55 of 300\n",
      "building tree 56 of 300\n",
      "building tree 57 of 300\n",
      "building tree 58 of 300\n",
      "building tree 59 of 300\n",
      "building tree 60 of 300\n",
      "building tree 61 of 300\n",
      "building tree 62 of 300\n",
      "building tree 63 of 300\n",
      "building tree 64 of 300\n",
      "building tree 65 of 300\n",
      "building tree 66 of 300\n",
      "building tree 67 of 300\n",
      "building tree 68 of 300\n",
      "building tree 69 of 300\n",
      "building tree 70 of 300\n",
      "building tree 71 of 300\n",
      "building tree 72 of 300\n",
      "building tree 73 of 300\n",
      "building tree 74 of 300\n",
      "building tree 75 of 300\n",
      "building tree 76 of 300\n",
      "building tree 77 of 300\n",
      "building tree 78 of 300\n",
      "building tree 79 of 300\n",
      "building tree 80 of 300\n",
      "building tree 81 of 300\n",
      "building tree 82 of 300\n",
      "building tree 83 of 300\n",
      "building tree 84 of 300\n",
      "building tree 85 of 300\n",
      "building tree 86 of 300\n",
      "building tree 87 of 300\n",
      "building tree 88 of 300\n",
      "building tree 89 of 300\n",
      "building tree 90 of 300\n",
      "building tree 91 of 300\n",
      "building tree 92 of 300\n",
      "building tree 93 of 300\n",
      "building tree 94 of 300\n",
      "building tree 95 of 300\n",
      "building tree 96 of 300\n",
      "building tree 97 of 300\n",
      "building tree 98 of 300\n",
      "building tree 99 of 300\n",
      "building tree 100 of 300\n",
      "building tree 101 of 300\n",
      "building tree 102 of 300\n",
      "building tree 103 of 300\n",
      "building tree 104 of 300\n",
      "building tree 105 of 300\n",
      "building tree 106 of 300\n",
      "building tree 107 of 300\n",
      "building tree 108 of 300\n",
      "building tree 109 of 300\n",
      "building tree 110 of 300\n",
      "building tree 111 of 300\n",
      "building tree 112 of 300\n",
      "building tree 113 of 300\n",
      "building tree 114 of 300\n",
      "building tree 115 of 300\n",
      "building tree 116 of 300\n",
      "building tree 117 of 300\n",
      "building tree 118 of 300\n",
      "building tree 119 of 300\n",
      "building tree 120 of 300\n",
      "building tree 121 of 300\n",
      "building tree 122 of 300\n",
      "building tree 123 of 300\n",
      "building tree 124 of 300\n",
      "building tree 125 of 300\n",
      "building tree 126 of 300\n",
      "building tree 127 of 300\n",
      "building tree 128 of 300\n",
      "building tree 129 of 300\n",
      "building tree 130 of 300\n",
      "building tree 131 of 300\n",
      "building tree 132 of 300\n",
      "building tree 133 of 300\n",
      "building tree 134 of 300\n",
      "building tree 135 of 300\n",
      "building tree 136 of 300\n",
      "building tree 137 of 300\n",
      "building tree 138 of 300\n",
      "building tree 139 of 300\n",
      "building tree 140 of 300\n",
      "building tree 141 of 300\n",
      "building tree 142 of 300\n",
      "building tree 143 of 300\n",
      "building tree 144 of 300\n",
      "building tree 145 of 300\n",
      "building tree 146 of 300\n",
      "building tree 147 of 300\n",
      "building tree 148 of 300\n",
      "building tree 149 of 300\n",
      "building tree 150 of 300\n",
      "building tree 151 of 300\n",
      "building tree 152 of 300\n",
      "building tree 153 of 300\n",
      "building tree 154 of 300\n",
      "building tree 155 of 300\n",
      "building tree 156 of 300\n",
      "building tree 157 of 300\n",
      "building tree 158 of 300\n",
      "building tree 159 of 300\n",
      "building tree 160 of 300\n",
      "building tree 161 of 300\n",
      "building tree 162 of 300\n",
      "building tree 163 of 300\n",
      "building tree 164 of 300\n",
      "building tree 165 of 300\n",
      "building tree 166 of 300\n",
      "building tree 167 of 300\n",
      "building tree 168 of 300\n",
      "building tree 169 of 300\n",
      "building tree 170 of 300\n",
      "building tree 171 of 300\n",
      "building tree 172 of 300\n",
      "building tree 173 of 300\n",
      "building tree 174 of 300\n",
      "building tree 175 of 300\n",
      "building tree 176 of 300\n",
      "building tree 177 of 300\n",
      "building tree 178 of 300\n",
      "building tree 179 of 300\n",
      "building tree 180 of 300\n",
      "building tree 181 of 300\n",
      "building tree 182 of 300\n",
      "building tree 183 of 300\n",
      "building tree 184 of 300\n",
      "building tree 185 of 300\n",
      "building tree 186 of 300\n",
      "building tree 187 of 300\n",
      "building tree 188 of 300\n",
      "building tree 189 of 300\n",
      "building tree 190 of 300\n",
      "building tree 191 of 300\n",
      "building tree 192 of 300\n",
      "building tree 193 of 300\n",
      "building tree 194 of 300\n",
      "building tree 195 of 300\n",
      "building tree 196 of 300\n",
      "building tree 197 of 300\n",
      "building tree 198 of 300\n",
      "building tree 199 of 300\n",
      "building tree 200 of 300\n",
      "building tree 201 of 300\n",
      "building tree 202 of 300\n",
      "building tree 203 of 300\n",
      "building tree 204 of 300\n",
      "building tree 205 of 300\n",
      "building tree 206 of 300\n",
      "building tree 207 of 300\n",
      "building tree 208 of 300\n",
      "building tree 209 of 300\n",
      "building tree 210 of 300\n",
      "building tree 211 of 300\n",
      "building tree 212 of 300\n",
      "building tree 213 of 300\n",
      "building tree 214 of 300\n",
      "building tree 215 of 300\n",
      "building tree 216 of 300\n",
      "building tree 217 of 300\n",
      "building tree 218 of 300\n",
      "building tree 219 of 300\n",
      "building tree 220 of 300\n",
      "building tree 221 of 300\n",
      "building tree 222 of 300\n",
      "building tree 223 of 300\n",
      "building tree 224 of 300\n",
      "building tree 225 of 300\n",
      "building tree 226 of 300\n",
      "building tree 227 of 300\n",
      "building tree 228 of 300\n",
      "building tree 229 of 300\n",
      "building tree 230 of 300\n",
      "building tree 231 of 300\n",
      "building tree 232 of 300\n",
      "building tree 233 of 300\n",
      "building tree 234 of 300\n",
      "building tree 235 of 300\n",
      "building tree 236 of 300\n",
      "building tree 237 of 300\n",
      "building tree 238 of 300\n",
      "building tree 239 of 300\n",
      "building tree 240 of 300\n",
      "building tree 241 of 300\n",
      "building tree 242 of 300\n",
      "building tree 243 of 300\n",
      "building tree 244 of 300\n",
      "building tree 245 of 300\n",
      "building tree 246 of 300\n",
      "building tree 247 of 300\n",
      "building tree 248 of 300\n",
      "building tree 249 of 300\n",
      "building tree 250 of 300\n",
      "building tree 251 of 300\n",
      "building tree 252 of 300\n",
      "building tree 253 of 300\n",
      "building tree 254 of 300\n",
      "building tree 255 of 300\n",
      "building tree 256 of 300\n",
      "building tree 257 of 300\n",
      "building tree 258 of 300\n",
      "building tree 259 of 300\n",
      "building tree 260 of 300\n",
      "building tree 261 of 300\n",
      "building tree 262 of 300\n",
      "building tree 263 of 300\n",
      "building tree 264 of 300\n",
      "building tree 265 of 300\n",
      "building tree 266 of 300\n",
      "building tree 267 of 300\n",
      "building tree 268 of 300\n",
      "building tree 269 of 300\n",
      "building tree 270 of 300\n",
      "building tree 271 of 300\n",
      "building tree 272 of 300\n",
      "building tree 273 of 300\n",
      "building tree 274 of 300\n",
      "building tree 275 of 300\n",
      "building tree 276 of 300\n",
      "building tree 277 of 300\n",
      "building tree 278 of 300\n",
      "building tree 279 of 300\n",
      "building tree 280 of 300\n",
      "building tree 281 of 300\n",
      "building tree 282 of 300\n",
      "building tree 283 of 300\n",
      "building tree 284 of 300\n",
      "building tree 285 of 300\n",
      "building tree 286 of 300\n",
      "building tree 287 of 300\n",
      "building tree 288 of 300\n",
      "building tree 289 of 300\n",
      "building tree 290 of 300\n",
      "building tree 291 of 300\n",
      "building tree 292 of 300\n",
      "building tree 293 of 300\n",
      "building tree 294 of 300\n",
      "building tree 295 of 300\n",
      "building tree 296 of 300\n",
      "building tree 297 of 300\n",
      "building tree 298 of 300\n",
      "building tree 299 of 300\n",
      "building tree 300 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   43.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(random_state=13,\n",
       "                                                    verbose=10),\n",
       "                   n_iter=13, n_jobs=4,\n",
       "                   param_distributions={'max_depth': [2, 4, 6, 8],\n",
       "                                        'min_samples_leaf': [5, 10, 15, 20, 25,\n",
       "                                                             30],\n",
       "                                        'min_samples_split': [10, 20, 30, 40,\n",
       "                                                              50],\n",
       "                                        'n_estimators': [100, 200, 300, 400]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest_w2v_sum.fit(word2vec_features_train_sum, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators parameter for the model above is 300\n",
      "The best min_samples_split parameter for the model above is 30\n",
      "The best min_samples_leaf parameter for the model above is 20\n",
      "The best max_depth parameter for the model above is 8\n"
     ]
    }
   ],
   "source": [
    "params = rforest_w2v_sum.best_params_\n",
    "\n",
    "for x, y in params.items():\n",
    "    print('The best', x, 'parameter for the model above is', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "#finding predicted probs for training data and using that to find AUC\n",
    "y_train_prob = rforest_w2v_sum.predict_proba(word2vec_features_train_sum)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "tw2v_sum_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#same as above, for validation data\n",
    "y_val_prob = rforest_w2v_sum.predict_proba(word2vec_features_val_sum)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "vw2v_sum_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The W2V training data set had an AUC score  of 0.8784761916406341 while the W2V validation data set had an AUC score of 0.6844633072202403 for a difference of: 0.19401288442039377\n"
     ]
    }
   ],
   "source": [
    "print('The W2V training data set had an AUC score  of', tw2v_sum_auc, \n",
    "      'while the W2V validation data set had an AUC score of', vw2v_sum_auc, 'for a difference of:', tw2v_sum_auc - vw2v_sum_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222</td>\n",
       "      <td>0.048640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196</td>\n",
       "      <td>0.047732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>0.044951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>0.038716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>0.032922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99</td>\n",
       "      <td>0.025540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>208</td>\n",
       "      <td>0.022918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>0.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>144</td>\n",
       "      <td>0.018726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>217</td>\n",
       "      <td>0.016289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26</td>\n",
       "      <td>0.016192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>0.015969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>96</td>\n",
       "      <td>0.013238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>0.012212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>234</td>\n",
       "      <td>0.012089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>158</td>\n",
       "      <td>0.011598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>155</td>\n",
       "      <td>0.011467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>275</td>\n",
       "      <td>0.010957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>256</td>\n",
       "      <td>0.010810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>250</td>\n",
       "      <td>0.010729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>65</td>\n",
       "      <td>0.010199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>0.009946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>0.009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>98</td>\n",
       "      <td>0.009116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>0.008588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>191</td>\n",
       "      <td>0.008050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>44</td>\n",
       "      <td>0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56</td>\n",
       "      <td>0.007036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var  importance\n",
       "0   222    0.048640\n",
       "1   196    0.047732\n",
       "2    76    0.044951\n",
       "3   111    0.038716\n",
       "4   231    0.032922\n",
       "5    99    0.025540\n",
       "6   208    0.022918\n",
       "7    17    0.019517\n",
       "8   144    0.018726\n",
       "9   217    0.016289\n",
       "10   26    0.016192\n",
       "11   70    0.015969\n",
       "12   96    0.013238\n",
       "13   30    0.012212\n",
       "14  234    0.012089\n",
       "15  158    0.011598\n",
       "16  155    0.011467\n",
       "17  275    0.010957\n",
       "18  256    0.010810\n",
       "19  250    0.010729\n",
       "20   65    0.010199\n",
       "21   51    0.009946\n",
       "22   21    0.009393\n",
       "23   98    0.009116\n",
       "24   14    0.008910\n",
       "25  100    0.008588\n",
       "26  191    0.008050\n",
       "27    2    0.007796\n",
       "28   44    0.007059\n",
       "29   56    0.007036"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 30 features\n",
    "blah = pd.DataFrame()\n",
    "blah['var'] = word2vec_features_train_sum.columns\n",
    "blah['importance'] = rforest_w2v_sum.best_estimator_.feature_importances_\n",
    "blah = blah.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "blah[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. 4) Which set of features performed the best? How else do you think you could improve this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From running random forests on the text mining and NLP methods above, the best performance came from using TF-IDF. Using TF-IDF, a validation AUC of .89 was acheived, which is quite higher than the other methods used. TF-IDF also had a small difference between training and validation AUC of 0.00766, showing that the model generalized quite well. \n",
    "\n",
    "### The performance of all models ran above could be improved by the expansion of the RandomizedSearchGrid; by allowing for increased n_estimators, min_samples_split, etc., the performance could be improved by getting closer to the optimal parameters. Additionally, I used a unigram for the words; testing out bigrams/trigrams/(skip gram for word2vec) on the models could see increased performance. For the Word2Vec methods, tuning the parameter of window_size parameter could give performance improvement. Specifically for the Word2Vec methods, I believe that a larger training set could also lead to higher performance. Lastly, for the word2vec models, the embeddings were used to find embedding averages and summations, but other features could be created by using min/max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Use window_size values 2-5 on W2V models; report AUC, give intuition on results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train2 = Word2Vec(x_train_clean, min_count = 5, size = 300, window = 2)\n",
    "word2vec_val2 = Word2Vec(x_val_clean, min_count = 5, size = 300, window = 2)\n",
    "\n",
    "word2vec_train3 = Word2Vec(x_train_clean, min_count = 5, size = 300, window = 3)\n",
    "word2vec_val3 = Word2Vec(x_val_clean, min_count = 5, size = 300, window = 3)\n",
    "\n",
    "word2vec_train4 = Word2Vec(x_train_clean, min_count = 5, size = 300, window = 4)\n",
    "word2vec_val4 = Word2Vec(x_val_clean, min_count = 5, size = 300, window = 4)\n",
    "\n",
    "word2vec_train5 = Word2Vec(x_train_clean, min_count = 5, size = 300, window = 5)\n",
    "word2vec_val5 = Word2Vec(x_val_clean, min_count = 5, size = 300, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:01<00:00, 8924.34it/s]\n",
      "100%|| 7500/7500 [00:00<00:00, 9523.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tx2 = train_avg_embedding(word2vec_train2)\n",
    "tx2 = pd.DataFrame(tx2)\n",
    "vx2 = val_avg_embedding(word2vec_val2)\n",
    "vx2 = pd.DataFrame(vx2)\n",
    "\n",
    "for x in (tx2, vx2):\n",
    "    x = (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset AUC is  0.898111720719728\n",
      "The validation dataset AUC is  0.5578340132639829\n"
     ]
    }
   ],
   "source": [
    "log = LGCV(random_state = 13, cv=5, penalty='l2', solver='liblinear', max_iter=1000, dual=False).fit(tx2, y_train)\n",
    "\n",
    "y_train_predprob = log.predict_proba(tx2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_predprob[:,1], pos_label = 1)\n",
    "print(\"The training dataset AUC is \", metrics.auc(fpr, tpr))\n",
    "\n",
    "y_val_predprob = log.predict_proba(vx2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_predprob[:,1], pos_label = 1)\n",
    "print(\"The validation dataset AUC is \", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:02<00:00, 7948.21it/s]\n",
      "100%|| 7500/7500 [00:00<00:00, 8569.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset AUC is  0.9026759085569309\n",
      "The validation dataset AUC is  0.5304178265609167\n"
     ]
    }
   ],
   "source": [
    "tx3 = train_avg_embedding(word2vec_train3)\n",
    "tx3 = pd.DataFrame(tx3)\n",
    "vx3 = val_avg_embedding(word2vec_val3)\n",
    "vx3 = pd.DataFrame(vx3)\n",
    "\n",
    "for x in (tx3, vx3):\n",
    "    x = (x - x.mean()) / x.std()\n",
    "\n",
    "log = log.fit(tx3, y_train)\n",
    "\n",
    "y_train_predprob = log.predict_proba(tx3)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_predprob[:,1], pos_label = 1)\n",
    "print(\"The training dataset AUC is \", metrics.auc(fpr, tpr))\n",
    "\n",
    "y_val_predprob = log.predict_proba(vx3)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_predprob[:,1], pos_label = 1)\n",
    "print(\"The validation dataset AUC is \", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:02<00:00, 7426.41it/s]\n",
      "100%|| 7500/7500 [00:01<00:00, 5357.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset AUC is  0.9054479897049451\n",
      "The validation dataset AUC is  0.6020738240500534\n"
     ]
    }
   ],
   "source": [
    "tx4 = train_avg_embedding(word2vec_train4)\n",
    "tx4 = pd.DataFrame(tx4)\n",
    "vx4 = val_avg_embedding(word2vec_val4)\n",
    "vx4 = pd.DataFrame(vx4)\n",
    "\n",
    "for x in (tx4, vx4):\n",
    "    x = (x - x.mean()) / x.std()\n",
    "\n",
    "log = log.fit(tx4, y_train)\n",
    "\n",
    "y_train_predprob = log.predict_proba(tx4)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_predprob[:,1], pos_label = 1)\n",
    "print(\"The training dataset AUC is \", metrics.auc(fpr, tpr))\n",
    "\n",
    "y_val_predprob = log.predict_proba(vx4)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_predprob[:,1], pos_label = 1)\n",
    "print(\"The validation dataset AUC is \", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17500/17500 [00:02<00:00, 8608.43it/s]\n",
      "100%|| 7500/7500 [00:00<00:00, 8082.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset AUC is  0.9070457369484739\n",
      "The validation dataset AUC is  0.5820775459554299\n"
     ]
    }
   ],
   "source": [
    "tx5 = train_avg_embedding(word2vec_train5)\n",
    "tx5 = pd.DataFrame(tx5)\n",
    "vx5 = val_avg_embedding(word2vec_val5)\n",
    "vx5 = pd.DataFrame(vx5)\n",
    "\n",
    "for x in (tx5, vx5):\n",
    "    x = (x - x.mean()) / x.std()\n",
    "\n",
    "log = log.fit(tx5, y_train)\n",
    "\n",
    "y_train_predprob = log.predict_proba(tx5)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_predprob[:,1], pos_label = 1)\n",
    "print(\"The training dataset AUC is \", metrics.auc(fpr, tpr))\n",
    "\n",
    "y_val_predprob = log.predict_proba(vx5)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_predprob[:,1], pos_label = 1)\n",
    "print(\"The validation dataset AUC is \", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best performing model of varying window_size is the model that uses window_size = 4. I would expect the model with the largest window_size to be the best performer, given it looks at more surrounding words to determine a word's context. Grading off of training AUC's, this logic is supported as the window_size=5 model has the highest training AUC, however, it does not have the best AUC for it's validation data. This could be because at a range of 4 words from the word predicted, those surrounding words could be more related, therefore allowing the model to generalize more accurately. At windowsize=5, a word 5 words away may be unrelated to the word we're trying to predict and lead to lower performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
